{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfPOGD095ohI"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D,  LeakyReLU, BatchNormalization, Activation, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import zipfile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpRq907uRlH9",
        "outputId": "85438c72-386c-4829-f787-3604bf66d5d9"
      },
      "outputs": [],
      "source": [
        "# Download the dataset saved on drive using ID\n",
        "!gdown \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqxPAxWHfDL"
      },
      "outputs": [],
      "source": [
        "# Drive dataset path\n",
        "dataset_path = '/content/drive/My Drive/Dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd4CjFJVHjtR"
      },
      "outputs": [],
      "source": [
        "def extract_zip(zip_path, extraction_path):\n",
        "    \"\"\"\n",
        "    Extracts the contents of a zip file to a specified extraction directory.\n",
        "\n",
        "    Parameters:\n",
        "    - zip_path (str): Path to the zip file to be extracted.\n",
        "    - extraction_path (str): Path to the directory where the contents will be extracted.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Create the extraction directory if it doesn't exist\n",
        "    os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beLKLrxXpgtK"
      },
      "outputs": [],
      "source": [
        "# Path to the zipped dataset file in Google Drive\n",
        "dataset_file_path = '/content/Dataset.zip'\n",
        "\n",
        "# Directory where you want to extract the images\n",
        "extraction_path = '/content/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGQ6y7Y5w2B5"
      },
      "outputs": [],
      "source": [
        "extract_zip(dataset_file_path,extraction_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ruRzOjMEfqD"
      },
      "outputs": [],
      "source": [
        "# Set dataset parameters\n",
        "target_size= (128,128)\n",
        "batch_size=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGrAay2M5ohP"
      },
      "outputs": [],
      "source": [
        "# Set train and val directories\n",
        "train_dir = '/content/Dataset/train'\n",
        "val_dir = '/content/Dataset/validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5bOJ03_5ohQ"
      },
      "outputs": [],
      "source": [
        "def is_image_file(filename):\n",
        "    \"\"\"\n",
        "    Checks if a given filename has a valid image extension (JPEG or PNG).\n",
        "\n",
        "    Parameters:\n",
        "    - filename (str): The name of the file to be checked.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the file has a valid image extension, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    return filename.lower().endswith((\".jpg\", \".png\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv8d38yb5ohR"
      },
      "outputs": [],
      "source": [
        "def load_images(directory, target_size):\n",
        "    \"\"\"\n",
        "    Loads images from a specified directory, resizes them to a target size, and returns a NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): Path to the directory containing the images.\n",
        "    - target_size (tuple): A tuple specifying the target size (height, width) to resize the images.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: A NumPy array containing the loaded and resized images.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    images = [np.array(Image.open(os.path.join(directory, filename)).resize(target_size))\n",
        "              for filename in os.listdir(directory) if is_image_file(filename)]\n",
        "\n",
        "    # Rescale the images to the range [0, 1]\n",
        "    images = np.array(images) / 255.0  # Rescale by dividing by 255.0\n",
        "    images = tf.cast(images, tf.float32)\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQbfXTRk5ohS"
      },
      "outputs": [],
      "source": [
        "# Load train and val images\n",
        "train_images = load_images(train_dir, target_size)\n",
        "val_images = load_images(val_dir, target_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GycP_WiTGZ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def apply_noise_image(image, min_scale=5, max_scale=25):\n",
        "    \"\"\"\n",
        "    Applies Gaussian noise to an input image with a random scale.\n",
        "\n",
        "    Parameters:\n",
        "    - image (numpy.ndarray): Input image.\n",
        "    - min_scale (float, optional): Minimum scaling factor for the noise intensity. Defaults to 5.\n",
        "    - max_scale (float, optional): Maximum scaling factor for the noise intensity. Defaults to 25.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: Noised image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a random scale within the specified range\n",
        "    scale = np.random.uniform(min_scale, max_scale)\n",
        "\n",
        "    # Generate Gaussian noise with the same shape as the image\n",
        "    noise = np.random.randn(*image.shape)\n",
        "\n",
        "    # Scale the noise to fit the [0, 1] range\n",
        "    noise = noise * (scale / 255.0)  # Assuming the original image was in the range [0, 255]\n",
        "\n",
        "    # Add the scaled noise to the image\n",
        "    noised_image = image + noise\n",
        "\n",
        "    # Ensure that the resulting image is still within [0, 1]\n",
        "    noised_image = np.clip(noised_image, 0, 1)\n",
        "\n",
        "    # Convert the image to float32\n",
        "    noised_image = tf.cast(noised_image, tf.float32)\n",
        "\n",
        "    return noised_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaK0ikjT5ohT"
      },
      "outputs": [],
      "source": [
        "# Apply noise to train and val images\n",
        "noised_train_images = np.array([apply_noise_image(image) for image in train_images])\n",
        "noised_val_images = np.array([apply_noise_image(image) for image in val_images])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bar5B2Ea5ohU"
      },
      "outputs": [],
      "source": [
        "def show_images(images, nmax=4):\n",
        "    \"\"\"\n",
        "    Visualizes a list of images.\n",
        "\n",
        "    Parameters:\n",
        "    - images (list): List of images to be visualized.\n",
        "    - nmax (int, optional): Maximum number of images to display. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # Create subplots based on the number of images to display\n",
        "    fig, ax = plt.subplots(ncols=min(len(images), nmax), figsize=(12, 4))\n",
        "\n",
        "    # Iterate through the images and display them\n",
        "    for i, axi in enumerate(ax.flat):\n",
        "        axi.imshow(images[i])\n",
        "        axi.axis('off')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3MP2jfh5ohV"
      },
      "outputs": [],
      "source": [
        "# Visualize noised images\n",
        "show_images(noised_train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui8nqwRtKPq5"
      },
      "outputs": [],
      "source": [
        "# Visualize train images\n",
        "show_images(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Thmttxl5ohX"
      },
      "outputs": [],
      "source": [
        "# Define img shape for training\n",
        "image_shape = target_size + (3,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ySOWFWaTaKY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_autoencoder(image_shape):\n",
        "    \"\"\"\n",
        "    Builds a Convolutional AutoEncoder model.\n",
        "\n",
        "    Parameters:\n",
        "    - image_shape (tuple): Shape of the input images (height, width, channels).\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.models.Model: Convolutional AutoEncoder model.\n",
        "    \"\"\"\n",
        "    # Weights initialization\n",
        "    initializer = GlorotNormal()\n",
        "\n",
        "    # Encoder\n",
        "    inputs = Input(shape=image_shape)\n",
        "    x = Conv2D(64, (3, 3), kernel_initializer=initializer, padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.02)(x)  # Leaky ReLU activation\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.02)(x)  # Leaky ReLU activation\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.02)(x)  # Leaky ReLU activation\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(128, (3, 3), padding='same')(encoded)\n",
        "    x = LeakyReLU(alpha=0.02)(x)  # Leaky ReLU activation\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.02)(x)  # Leaky ReLU activation\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.02)(x)  # Leaky ReLU activation\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(3, (3, 3), activation='linear', padding='same')(x)\n",
        "\n",
        "    # AutoEncoder Model\n",
        "    autoencoder = tf.keras.models.Model(inputs, decoded)\n",
        "\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_epxXBT5ohZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def ssim_metric(original, reconstructed):\n",
        "    \"\"\"\n",
        "    Compute Structural Similarity Index (SSIM) between two images.\n",
        "\n",
        "    SSIM is a metric that measures the similarity between two images, taking into account luminance, contrast,\n",
        "    and structure. It returns a value between -1 and 1, where 1 indicates identical images.\n",
        "\n",
        "    Parameters:\n",
        "    - original (tf.Tensor): The original image.\n",
        "    - reconstructed (tf.Tensor): The reconstructed image.\n",
        "\n",
        "    Returns:\n",
        "    - float: SSIM value between the original and reconstructed images.\n",
        "    \"\"\"\n",
        "    ssim = tf.image.ssim(original, reconstructed, max_val=1.0)\n",
        "    return ssim\n",
        "\n",
        "def psnr_metric(img1, img2):\n",
        "    \"\"\"\n",
        "    Compute Peak Signal-to-Noise Ratio (PSNR) between two images.\n",
        "\n",
        "    PSNR is a metric that measures the quality of the reconstructed image compared to the original image.\n",
        "    It is expressed in decibels (dB), and higher values indicate better image quality.\n",
        "\n",
        "    Parameters:\n",
        "    - img1 (tf.Tensor): The original image.\n",
        "    - img2 (tf.Tensor): The reconstructed image.\n",
        "\n",
        "    Returns:\n",
        "    - float: PSNR value between the original and reconstructed images.\n",
        "    \"\"\"\n",
        "    psnr = tf.image.psnr(img1, img2, max_val=1.0)\n",
        "    return psnr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5xOq1eL5ohZ",
        "outputId": "7ce9679c-909b-4aff-d650-204e55809623"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "autoencoder= build_autoencoder(image_shape)\n",
        "# Compile the model\n",
        "# Select the optimizer, loss function and metrics for the model\n",
        "learning_rate=0.0001\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error',metrics=[ssim_metric, psnr_metric])\n",
        "\n",
        "# Print model summary\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRUjEUTeCxnL"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "batch_size=32\n",
        "n_epochs=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8c-YNS4L1M"
      },
      "outputs": [],
      "source": [
        "del autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiIhXzBG5ohb",
        "outputId": "9716c375-5850-46a9-ccae-fdd4e7ee4028"
      },
      "outputs": [],
      "source": [
        "# Train AUtoEncoder\n",
        "history = autoencoder.fit(noised_train_images, train_images, batch_size=batch_size, epochs=n_epochs,validation_data=(noised_val_images, val_images) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmYXaz06pgtd"
      },
      "outputs": [],
      "source": [
        "# Get reconstructed validation images from AutoEncoder\n",
        "reconstructed_images = autoencoder.predict(val_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukcRWeimpgtd"
      },
      "outputs": [],
      "source": [
        "def score_autoencoder(val_images, reconstructed_images):\n",
        "    \"\"\"\n",
        "    Calculates SSIM and PSNR scores between validation images and reconstructed images.\n",
        "\n",
        "    Parameters:\n",
        "    - val_images (numpy.ndarray): Validation images.\n",
        "    - reconstructed_images (numpy.ndarray): Reconstructed images.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average SSIM (Structural Similarity Index).\n",
        "    - float: Average PSNR (Peak Signal-to-Noise Ratio).\n",
        "    \"\"\"\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "\n",
        "    # Compute SSIM and PSNR between val images and reconstructed validation images\n",
        "    for i in range(len(val_images)):\n",
        "        ssim_score = ssim_metric(val_images[i], reconstructed_images[i])\n",
        "        psnr_score = psnr_metric(val_images[i], reconstructed_images[i])\n",
        "        ssim_scores.append(ssim_score)\n",
        "        psnr_scores.append(psnr_score)\n",
        "\n",
        "    # Calculate average SSIM and PSNR\n",
        "    average_ssim = np.mean(ssim_scores)\n",
        "    average_psnr = np.mean(psnr_scores)\n",
        "\n",
        "    return average_ssim, average_psnr, ssim_scores, psnr_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulHZA8Bv5ohc"
      },
      "outputs": [],
      "source": [
        "# Compute SSIM and PSNR scores between validation images and reconstructed images from validation images\n",
        "average_ssim, average_psnr, ssim_scores, psnr_scores = score_autoencoder(val_images, reconstructed_images)\n",
        "print(\"Average SSIM:\", average_ssim)\n",
        "print(\"Average PSNR:\", average_psnr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr2kVw9pgtf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_autoencoder_output(noised_val_images, reconstructed_images, val_images, save_path=\"Comparisons.png\"):\n",
        "    \"\"\"\n",
        "    Visualizes the outputs of image processing.\n",
        "\n",
        "    Parameters:\n",
        "    - noised_val_images (numpy.ndarray): Noised validation images.\n",
        "    - reconstructed_images (numpy.ndarray): Reconstructed images.\n",
        "    - val_images (numpy.ndarray): Original validation images.\n",
        "    - save_path (str, optional): Path to save the visualization. Defaults to \"Comparisons.png\".\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    n = len(noised_val_images)\n",
        "\n",
        "    # Increase the figure size\n",
        "    plt.figure(figsize=(40, 40))\n",
        "\n",
        "    for i in range(n):\n",
        "        # Display noisy images\n",
        "        ax = plt.subplot(3, n, i + 1)\n",
        "        plt.imshow(noised_val_images[i])\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        # Display denoised images by convolutional autoencoder\n",
        "        ax = plt.subplot(3, n, i + 1 + n)  # Adjust the index for the second row\n",
        "        plt.imshow(reconstructed_images[i])\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        # Display original images for comparison purposes\n",
        "        ax = plt.subplot(3, n, i + 1 + 2*n)  # Adjust the index for the third row\n",
        "        plt.imshow(val_images[i])\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Reduce spacing between images\n",
        "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    # Save the visualization\n",
        "    plt.savefig(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THQBU3lP5ohd"
      },
      "outputs": [],
      "source": [
        "# Visualize the outputs of the autoencoder\n",
        "show_autoencoder_output(noised_val_images, reconstructed_images, val_images, save_path=\"Comparisons.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Y1O-Xjpgth"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_loss_curves(history):\n",
        "    \"\"\"\n",
        "    Plots the training and validation loss curves.\n",
        "\n",
        "    Parameters:\n",
        "    - history (tf.keras.callbacks.History): Training history of the model.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3beoHjXD5oiF"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss curves\n",
        "plot_loss_curves(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBnShL0mpgtj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_loss_and_metrics(history):\n",
        "    \"\"\"\n",
        "    Plots training and validation loss curves, SSIM curve, and PSNR curve.\n",
        "\n",
        "    Parameters:\n",
        "    - history (tf.keras.callbacks.History): Training history of the model.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Create subplots for loss and metrics\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "\n",
        "    # Plot the training and validation loss curves\n",
        "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss Curves')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot the SSIM\n",
        "    axes[1].plot(history.history['ssim_metric'], label='SSIM')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('SSIM')\n",
        "    axes[1].set_title('SSIM Curve')\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Plot the PSNR\n",
        "    axes[2].plot(history.history['psnr_metric'], label='PSNR')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('PSNR')\n",
        "    axes[2].set_title('PSNR Curve')\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3N6NyG3bIr1"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss curves, SSIM curve, and PSNR curve\n",
        "plot_loss_and_metrics(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsP4_7Uopgtl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_ssim_and_psnr(ssim_scores, psnr_scores):\n",
        "    \"\"\"\n",
        "    Creates a graph to visualize SSIM and PSNR values over epochs.\n",
        "\n",
        "    Parameters:\n",
        "    - epochs (range): Range of epochs.\n",
        "    - ssim_scores (list): List of SSIM scores.\n",
        "    - psnr_scores (list): List of PSNR scores.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    epochs = range(len(ssim_scores))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot SSIM values\n",
        "    plt.plot(epochs, ssim_scores, label='SSIM', marker='o', linestyle='-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot PSNR values\n",
        "    plt.twinx()\n",
        "    plt.plot(epochs, psnr_scores, label='PSNR', marker='x', linestyle='--', color='tab:orange')\n",
        "    plt.ylabel('PSNR')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.title('SSIM and PSNR Comparison')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfB2sWehtlj2"
      },
      "outputs": [],
      "source": [
        "# Plot SSIM and PSNR values over epochs\n",
        "plot_ssim_and_psnr(ssim_scores, psnr_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4_64JG9Axa9"
      },
      "outputs": [],
      "source": [
        "autoencoder.save('denoising_autoencoder.keras')\n",
        "# Save trained model\n",
        "autoencoder.save('denoising_autoencoder.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzAjT07gxK_5"
      },
      "outputs": [],
      "source": [
        "autoencoder.save('AutoEncoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EWUCigU2nns"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "shutil.make_archive('AutoEncoder', 'zip', 'AutoEncoder)\n",
        "\n",
        "files.download(\"AutoEncoder.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwh2npaq2t6h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
